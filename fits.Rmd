---
title: "Model fits"
output: html_document
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "figs-knitr/",
  cache.path = "cache/",
  cache = TRUE,
  autodep = TRUE
)
```

We'll start by loading some packages and reading in a number of functions that are included in `zoib-functions.R`:

```{r functions, cache=FALSE, warning=FALSE,message=FALSE,results='hide'}
source("zoib-functions.R")
library(tidyverse)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
sm <- stan_model("zoib1re.stan")
```

The following is an example of a ZOIB mixed-effects model with RdNBR as the predictor and basal area killed by fire as the response. There is a random intercept for each fire.

```{r mainfit, cache=TRUE, dependson="functions",warning=FALSE,message=FALSE}
d <- get_dat(Firemort.BA.p, RdNBR, 
  file = "ALL_SHOBS_2010_2014_PLOTS_MASTER_w_CO_FINAL_20171029.csv",
  max_predictor = 2500)
m <- fit_model(d, model = sm, predictors = "xscaled")
print(m$model, probs = c(0.05, 0.5, 0.95), pars = "log_lik", include = FALSE)
p <- make_predictions(d = d, f = m$f, model = m$model)
g <- make_pred_plot(p, d)
print(g)
```

Each thin line is the prediction for an individual fire (incorporates the random intercepts). The thick line is the median global prediction and the shaded region shows the 95% credible interval.

Let's make an ROC plot for range of possible thresholds from 0.2 to 0.8 on the response. The y-axis represents true positive rate and the x-axis represents false positive rate. 

```{r}
predictions <- make_predictions_dat(model = m$model, f = m$f, d = d)
make_roc(d = d, predictions = predictions)
```

LOOIC is analogous to AIC:

```{r}
library(loo)
extract_log_lik(m$model) %>% loo()
```

Let's inspect the residuals plotted against the predictor, other possible predictors, and against the fitted values (`pred`).

```{r residuals1, warning=FALSE}
mutate(d, pred = predictions, resid = pred - y) %>% 
  select(-y, -group, -xscaled, -yp, -y1, -y0) %>% 
  reshape2::melt(id.vars = "resid") %>%
  ggplot(aes(value, resid)) + 
  geom_point(alpha = 0.6) +
  facet_wrap(~variable, scales = "free_x") +
  ggsidekick::theme_sleek() +
  geom_hline(yintercept = 0, lty = 2, col = "blue")
```

# Interactions

Now let's try a model that includes other covariates and their interaction with RdNBR. At this point I've left out a couple of the predictors you used that have NAs for some rows of data. I think there may be an issue where all the locations with zero burned have NAs for a predictor.

```{r interactions, cache=TRUE, dependson="functions", warning=FALSE,message=FALSE}
sm <- stan_model("zoib1re.stan")
d <- mutate(d, ba_ha = arm::rescale(ba_ha),
  northing = arm::rescale(northing),
  easting = arm::rescale(easting),
  elev = arm::rescale(elev),
  heatload = arm::rescale(heatload),
  slope = arm::rescale(slope),
  aspect = arm::rescale(aspect)
)
m <- fit_model(d, model = sm,
  predictors = c("xscaled", "northing*xscaled", "elev*xscaled", 
    "easting*xscaled", "slope*xscaled", "heatload*xscaled", 
    "aspect*xscaled"))
extract_log_lik(m$model) %>% loo()
```

LOOIC supports this more fully parameterized model over the simple model.

The AUC also gets higher:

```{r}
predictions <- make_predictions_dat(model = m$model, f = m$f, d = d)
roc_all <- make_roc(d = d, predictions = predictions)
roc_all
```

Again let's look at the residuals:

```{r residuals2, warning=FALSE}
mutate(d, pred = predictions, resid = pred - y) %>% 
  select(-y, -group, -xscaled, -yp, -y1, -y0) %>% 
  reshape2::melt(id.vars = "resid") %>%
  ggplot(aes(value, resid)) + 
  geom_point(alpha = 0.6) +
  facet_wrap(~variable, scales = "free_x") +
  ggsidekick::theme_sleek() +
  geom_hline(yintercept = 0, lty = 2, col = "blue")
```

Let's plot out the main effect coefficients and the interaction coefficients. This takes a little bit of data manipulation first. The dots are medians, the thick lines are 50% credible intervals, and the thin lines are 95% credible intervals.

```{r plot-interactions, cache=TRUE, dependson=c("interactions", "functions"), fig.height=6.5}
terms <- tibble(
  name = colnames(m$stan_dat$Xp_ij),
  num = as.character(seq_along(colnames(m$stan_dat$Xp_ij)))
)

models <- tibble(model = c("0", "1", "p"), 
  model_full = factor(
    c("Pr(not 0)", "Pr(1)", "Proportion"),
    levels = 
      c("Pr(not 0)", "Proportion", "Pr(1)")
  ))

b <- broom::tidyMCMC(m$model, estimate.method = "median", conf.int = TRUE)
b_inner <- broom::tidyMCMC(m$model, estimate.method = "median", conf.int = TRUE,
  conf.level = 0.5) %>% 
  rename(conf.low.25 = conf.low, conf.high.75 = conf.high) %>% 
  select(-estimate, -std.error)

b <- b %>% inner_join(b_inner, by = "term") %>% 
  filter(grepl("b[01p]+_j", term)) %>% 
  filter(!grepl("\\[1\\]", term)) %>% 
  mutate(num = gsub("b[01p]+_j\\[([0-9]+)\\]", "\\1", term)) %>% 
  mutate(model = gsub("b([01p]+)_j\\[([0-9]+)\\]", "\\1", term)) %>% 
  left_join(terms, by = "num") %>% 
  mutate(name = gsub("xscaled", " RdNBR", name))

pal <- RColorBrewer::brewer.pal(4, "Set2")
pal <- c(pal[1], "grey50", pal[2])
b %>% 
  mutate(estimate = ifelse(model == 0, -estimate, estimate)) %>% 
  mutate(conf.low = ifelse(model == 0, -conf.low, conf.low)) %>%
  mutate(conf.high = ifelse(model == 0, -conf.high, conf.high)) %>% 
  mutate(conf.high.75 = ifelse(model == 0, -conf.high.75, conf.high.75)) %>% 
  mutate(conf.low.25 = ifelse(model == 0, -conf.low.25, conf.low.25)) %>% 
  mutate(interaction = grepl(":", name)) %>% 
  inner_join(models, by = "model") %>% 
  mutate(xvar = fct_relevel(name, " RdNBR", after = Inf)) %>% 
  ggplot(aes(y = estimate, x = xvar, 
    colour = model_full, shape = interaction)) +
  geom_hline(yintercept = 0, lty = 2, col = "grey55") +
  geom_linerange(aes(ymin = conf.low.25, ymax = conf.high.75),
    position = position_dodge(width = 0.4), size = 1.2) +
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high),
    position = position_dodge(width = 0.4)) +
  xlab("") + 
  ggsidekick::theme_sleek() +
  coord_flip() +
  scale_color_manual(values = pal) +
  scale_shape_manual(values = c(19, 21)) +
  labs(colour = "Model", shape = "Interaction", y = "Coefficient value")
```

I haven't done it here yet, but another way to represent these effects would be to show their effect on the predictions holding the other variables at their mean values. It would look like the first plot in this document with one of these predictors on the x-axis and likely multiple panels. Otherwise, the coefficients themselves have to be kept separate. 

# Cross-validation

One way to cross validate the model fits — perhaps the most realistic way — is to build them leaving out a fire and then try to predict the basal area killed by a fire for that left out fire. We can also gather all those predictions and create a out-of-sample ROC plot.

In the following plot, the predictions come from a model excluding that fire. 

```{r cv-model, cache=TRUE, dependson=c("functions"), warning=FALSE, message=FALSE, results='hide'}
d <- get_dat(Firemort.BA.p, RdNBR,
  file = "ALL_SHOBS_2010_2014_PLOTS_MASTER_w_CO_FINAL_20171029.csv",
  max_predictor = 2500)
fires <- unique(d$group)
models <- map(fires, function(x) {
  fit_model(d = d[d$group != x, , drop = FALSE], model = sm, 
    predictors = "xscaled", iter = 1000, chains = 1)
})
preds <- map2(models, fires, function(x, y) {
  make_predictions(d = d[d$group == y, , drop = FALSE], f = x$f, 
    model = x$model, re = FALSE)
})
```

Plot:

```{r, cache=FALSE, dependson=c("cv-model", "functions"), fig.height=6.25, fig.width=8}
preds_df <- bind_rows(preds) %>% rename(group_id = group) %>% 
  mutate(x = xscaled * 2 * sd(d$x) + mean(d$x)) %>% 
  inner_join(select(d, group, group_id) %>% unique(), by = "group_id")
ggplot(preds_df, aes(x, est)) +
  geom_ribbon(alpha = 0.5, fill = "grey20", aes(ymin = lwr, ymax = upr)) +
  geom_point(data = d, aes(x, y), alpha = 0.6) +
  geom_line(lwd = 1.2, col = "red") +
  ylim(0, 1) +
  ggsidekick::theme_sleek() +
  ylab("Proportion burned") +
  xlab("Predictor value") +
  labs(colour = "Fire location") +
  facet_wrap(~group)
```

```{r}
roc_dat <- map2_df(models, fires, function(x, y) {
  dd <- d[d$group == y, ]
  dd$prediction <- make_predictions_dat(model = x$model, f = x$f, d = dd)
  dd
})
make_roc(roc_dat, predictions = roc_dat$prediction)
```

# NLS comparison

Let's compare our model fits to those obtained from a nonlinear least-squares model.

```{r nls, cache=TRUE}
d <- get_dat(Firemort.BA.p, RdNBR,
  file = "ALL_SHOBS_2010_2014_PLOTS_MASTER_w_CO_FINAL_20171029.csv",
  max_predictor = 2500)
iForm <- y~1/(1+exp(a*(b-x)))
resNLS <- nls(iForm, data = d, start = list(a = 0.005, b = 375))
d$nls_fit <- predict(resNLS, newdata = d)
make_roc(d = d, predictions = d$nls_fit)

m <- fit_model(d, model = sm, predictors = "xscaled")
p <- make_predictions(d = d, f = m$f, model = m$model)
p$nls_fit <- predict(resNLS, newdata = p)
g <- make_pred_plot(p, d)
g + geom_line(aes(y = nls_fit), col = "red", lwd = 2, lty = 2)

d$stan_fit <- make_predictions_dat(m$m, d, m$f)  
```

In the above plot, the thick red line is the projection from NLS.

So, we get a similar fit with NLS but we end up over-predicting our response when it gets close to 0.

I was trying to come up with some other way to compare the models. One way would be with a binned comparison where you split the data in bins from left to right and calculate the mean of the response in each bin along with the mean of the predictions. We can see that the NLS model misses the boat for low values of the response probably because it's getting driven more by the squared residuals towards the middle of the response. 

```{r binned}
cuts <- seq(min(d$x), max(d$x), length.out = 11)
d$cut <- cuts[findInterval(d$x, cuts)]
group_by(d, cut) %>% 
  summarise(mean_y = mean(y), mean_stan = mean(stan_fit), mean_nls = mean(nls_fit)) %>%
  reshape2::melt(id = "cut") %>% 
  ggplot(aes(cut, value, colour = variable)) +
  geom_line() +
  ggsidekick::theme_sleek() +
  scale_colour_manual(values = c("mean_y" = "grey40", "mean_stan" = "red", "mean_nls" = "blue")) +
  geom_point(data = d, aes(x = x, y = y), inherit.aes = FALSE, alpha = 0.4) +
  geom_vline(aes(xintercept = cut), alpha = 0.5)
```

# Fit all

We can easily iterate over all the response variables. Here I'm doing that with RdNBR as the predictor in all cases. I'm not doing the cross validation here although that will be easy to insert.

```{r fit-all-rdnbr, cache=TRUE}
y_vars <- c(
  "Firemort.BA.p",
  "Firemort.trees.p",
  "CHARHT_percMax",
  "BOLESCORCH",
  "CHARCOV")

out <- map(y_vars, function(y) {
  d <- get_dat(!!quo_name(y), RdNBR,
    file = "ALL_SHOBS_2010_2014_PLOTS_MASTER_w_CO_FINAL_20171029.csv",
    max_predictor = 2500)
  f <- y~1/(1+exp(a*(b-x)))
  resNLS <- nls(f, data = d, start = list(a = 0.005, b = 375))
  d$nls_fit <- predict(resNLS, newdata = d)
  m <- fit_model(d, model = sm, predictors = "xscaled", iter = 600, chains = 4)
  p <- make_predictions(d = d, f = m$f, model = m$model)
  p$nls_fit <- predict(resNLS, newdata = p)
  p$response <- y
  g <- make_pred_plot(p, d) + geom_line(aes(y = nls_fit), col = "red", lwd = 2, lty = 2)
  list(g = g, p = p, m = m, d = d, nls_model = resNLS)
})
```

In the following plot the green is from NLS fits and the black is the ZOIB-RE model.

```{r plot-all-rdnbr}
preds <- map_df(out, function(x) x$p)

ggplot(preds, aes(x, est, ymin = lwr, ymax = upr)) +
  geom_point(data = d, aes(x, y),
    inherit.aes = FALSE, alpha = 0.5) +
  geom_line(lwd = 1.2) +
  geom_ribbon(alpha = 0.5, fill = "grey20") +
  ylim(0, 1) +
  ggsidekick::theme_sleek() +
  ylab("Proportion burned") +
  xlab("Predictor value") +
  facet_wrap(~response) +
  geom_line(aes(y = nls_fit), col = "green", lty = 1, lwd = 1.1) +
  geom_line(aes(y = est_re, group = as.factor(group)),
    col = "grey50", lty = 1, lwd = 0.2)
```

We can calculate the ROC/AUC for all the response variables. Interestingly the AUC/ROC is identical or almost identical between NLS and ZOIB. I think this is because of the nature of ROC where the response has to get cut into above or below a threshold for comparison and so as long as the slope is about the same and the shape is the same (logit) you get the same ROC/AUC. So the advantage of the ZOIB model isn't in ROC/AUC.

```{r rocs-all, fig.height=9, fig.width=9}
rocs <- map(out, function(x) {
  predictions <- make_predictions_dat(model = x$m$model, f = x$m$f, d = x$d)
  make_roc(d = x$d, predictions = predictions) +
    ggtitle(unique(x$p$response))
})
library(gridExtra)
n <- length(rocs)
nCol <- floor(sqrt(n))
do.call("grid.arrange", c(rocs, ncol=nCol))

rocs <- map(out, function(x) {
  predictions <- predict(x$nls_model, newdata = x$d)
  make_roc(d = x$d, predictions = predictions) +
    ggtitle(unique(x$p$response))
})
n <- length(rocs)
nCol <- floor(sqrt(n))
do.call("grid.arrange", c(rocs, ncol=nCol))
```

# Random thoughts


The advantages to my eyes are:

- properly representing the data-generating process
- better fit to the data for the zeros
- accurately estimating coefficients with probability intervals (i.e. the interaction model)
- if we made the map predictions with the ZOIB model we can easily create credible interval versions. E.g. The lower 10% version and 90% version.
- we can easily quantify the uncertainty around predictions used for other purposes
- separating the contributions of predictors into the effects on 0 vs. non-zero, proportion, and non-1 vs. 1
- methodologically it's quite novel --- I haven't seen this applied anywhere in ecology before

The disadvantages are:

- it's more complicated (for similar ROC curves)
- you don't get one simple equation that people can use to predict from (but we could make a really simple R package that people could plug their own data into to predict with)... there is also one equation for just the mean prediction, but it includes 3 logistic curves so it's not nearly as simple to write out
- methodologically it's novel rather than doing what people in the field are used to

The model could be made more complicated by adding things like random slopes or group-level predictors on the random intercept, e.g. mean easting of the fire... but such a model would be less useful for making predictions in totally new places.
